{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra notebook - Calculations of carconstricted network metrics\n",
    "## Project: Growing Urban Bicycle Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook supplements the main analysis with calculations on the car networks constricted by the bicycle network growth. This notebook was run in a separate environment from all other notebooks, therefore it does not match the repository's folder structure and defines its own functions, for example.\n",
    "\n",
    "Contact: Sayat Mimar (smimar@ur.rochester.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_names = ['amsterdam','barcelona','bath','bern','birmingham','boston','bradford','budapest','buenosaires','chicago',\n",
    "             'cologne','copenhagen','delft','detroit','edinburgh','glasgow','hongkong','kathmandu','leeds','luanda','malmo',\n",
    "              'manhattan',  'marrakesh','milan','moscow','mumbai','munich','oslo','paris','philadelphia','rabat','sanfrancisco',\n",
    "              'santiago','shahalam','sheffield','singapore','stuttgart','tashkent','telaviv','turin','ulaanbaatar','vienna'\n",
    "              ,'zurich'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = ['0.025',\n",
    " '0.050',\n",
    " '0.075',\n",
    " '0.100',\n",
    " '0.125',\n",
    " '0.150',\n",
    " '0.175',\n",
    " '0.200',\n",
    " '0.225',\n",
    " '0.250',\n",
    " '0.275',\n",
    " '0.300',\n",
    " '0.325',\n",
    " '0.350',\n",
    " '0.375',\n",
    " '0.400',\n",
    " '0.425',\n",
    " '0.450',\n",
    " '0.475',\n",
    " '0.500',\n",
    " '0.525',\n",
    " '0.550',\n",
    " '0.575',\n",
    " '0.600',\n",
    " '0.625',\n",
    " '0.650',\n",
    " '0.675',\n",
    " '0.700',\n",
    " '0.725',\n",
    " '0.750',\n",
    " '0.775',\n",
    " '0.800',\n",
    " '0.825',\n",
    " '0.850',\n",
    " '0.875',\n",
    " '0.900',\n",
    " '0.925',\n",
    " '0.950',\n",
    " '0.975',\n",
    " '1.000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots_1 = ['all',\n",
    " '0.025',\n",
    " '0.050',\n",
    " '0.075',\n",
    " '0.100',\n",
    " '0.125',\n",
    " '0.150',\n",
    " '0.175',\n",
    " '0.200',\n",
    " '0.225',\n",
    " '0.250',\n",
    " '0.275',\n",
    " '0.300',\n",
    " '0.325',\n",
    " '0.350',\n",
    " '0.375',\n",
    " '0.400',\n",
    " '0.425',\n",
    " '0.450',\n",
    " '0.475',\n",
    " '0.500',\n",
    " '0.525',\n",
    " '0.550',\n",
    " '0.575',\n",
    " '0.600',\n",
    " '0.625',\n",
    " '0.650',\n",
    " '0.675',\n",
    " '0.700',\n",
    " '0.725',\n",
    " '0.750',\n",
    " '0.775',\n",
    " '0.800',\n",
    " '0.825',\n",
    " '0.850',\n",
    " '0.875',\n",
    " '0.900',\n",
    " '0.925',\n",
    " '0.950',\n",
    " '0.975',\n",
    " '1.000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in city_names:\n",
    "    \n",
    "    graphs=[]\n",
    "    eff_global = []\n",
    "    eff_local=[]\n",
    "    clustering_10=[]\n",
    "    clustering_5=[]\n",
    "    clustering_3=[]\n",
    "    anisotropy_10=[]\n",
    "    anisotropy_5=[]\n",
    "    anisotropy_3=[]\n",
    "    directness=[]\n",
    "    \n",
    "     ##### read the _carall files\n",
    "    with gzip.open('all_cities_3_1/'+city+'/'+city+'_carall.picklez', 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "        (graph.es())['weight'] = np.array((graph.es())['weight'])/1000\n",
    "\n",
    "    graphs.append(graph)\n",
    "    ##### read all snapshot files\n",
    "    for s in snapshots:\n",
    "\n",
    "        with gzip.open('all_cities_3_1/'+city+'/'+city+'_carconstrictedbike_poi_railwaystation_Cq'+s+'.picklez', 'rb') as f:\n",
    "            graph = pickle.load(f)\n",
    "            (graph.es())['weight'] = np.array((graph.es())['weight'])/1000\n",
    "            #graph.delete_edges(11910)\n",
    "        graphs.append(graph)\n",
    "        \n",
    "        \n",
    "    btwnness = []\n",
    "    count=0\n",
    "    for k in graphs:\n",
    "        btwnness.append(k.betweenness(weights='weight',nobigint=False))\n",
    "        count+=1\n",
    "        if count%10==0:\n",
    "            print(count)\n",
    "        \n",
    "    pd_btw = pd.DataFrame()\n",
    "    pd_btw['ids'] = graphs[0].vs()['id']\n",
    "    c=0\n",
    "    for j in snapshots:\n",
    "        pd_btw[j] = btwnness[c]\n",
    "        graphs[c].vs['betw'] = btwnness[c]\n",
    "        c+=1  \n",
    "    pd_btw.to_csv('results_3/betweenness_rail_c/'+city+'_carconstrictedbike_poi_railwaystation_closeness.csv'+'_betwnns.csv')  ### location of the csv file, write betweenness into csv file\n",
    "    \n",
    "    for gr in graphs:\n",
    "        eff_global.append(calculate_efficiency_global(gr))\n",
    "        eff_local.append(calculate_efficiency_local(gr))\n",
    "        clustering_10.append(center_drift_weighted(gr,90))\n",
    "        clustering_5.append(center_drift_weighted(gr,95))\n",
    "        clustering_3.append(center_drift_weighted(gr,97))\n",
    "        anisotropy_10.append(bet_anisotropy_weighted(gr,90))\n",
    "        anisotropy_5.append(bet_anisotropy_weighted(gr,95))\n",
    "        anisotropy_3.append(bet_anisotropy_weighted(gr,97))\n",
    "        directness.append(calculate_directness(gr))\n",
    "\n",
    "\n",
    "        \n",
    "    pd_metrics = pd.DataFrame()\n",
    "\n",
    "    pd_metrics['snapshots'] = snapshots\n",
    "    metrics = ['eff_global','eff_local','clustering_10','clustering_5','clustering_3','anisotropy_10','anisotropy_5','anisotropy_3','directness']\n",
    "    values = [eff_global,eff_local,clustering_10,clustering_5,clustering_3,anisotropy_10,anisotropy_5,anisotropy_3,directness]\n",
    "\n",
    "    c_2 = 0\n",
    "    for m in metrics:\n",
    "\n",
    "        pd_metrics[m] = values[c_2]\n",
    "        c_2+=1\n",
    "        \n",
    "    pd_metrics.to_csv('results_3/metrics_rail_c/'+city+'_carconstrictedbike_poi_railwaystation_closeness.csv') ##write metric results into csv file\n",
    "    \n",
    "    print(city)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_cities = ['tokyo','helsinki','berlin','hamburg','manchestergreater','london','losangeles','mexico','rome','houston','jakarta',\n",
    "             'karachi','phoenix','saopaulo'  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = ['0.500','1.000']\n",
    "snapshots_1 = ['all','0.500','1.000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in big_cities:\n",
    "    \n",
    "    graphs=[]\n",
    "    eff_global = []\n",
    "    eff_local=[]\n",
    "    clustering_10=[]\n",
    "    clustering_5=[]\n",
    "    clustering_3=[]\n",
    "    anisotropy_10=[]\n",
    "    anisotropy_5=[]\n",
    "    anisotropy_3=[]\n",
    "    directness=[]\n",
    "    \n",
    "    \n",
    "    with gzip.open('all_cities_3_1/'+city+'/'+city+'_carall.picklez', 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "        (graph.es())['weight'] = np.array((graph.es())['weight'])/1000\n",
    "        \n",
    "    graphs.append(graph)\n",
    "\n",
    "    for s in snapshots:\n",
    "\n",
    "        with gzip.open('all_cities_3_1/'+city+'/'+city+'_carconstrictedbike_poi_railwaystation_Rq'+s+'.picklez', 'rb') as f:\n",
    "            graph = pickle.load(f)\n",
    "            (graph.es())['weight'] = np.array((graph.es())['weight'])/1000\n",
    "            #graph.delete_edges(11910)\n",
    "        graphs.append(graph)\n",
    "        \n",
    "        \n",
    "    btwnness = []\n",
    "    count=0\n",
    "    for k in graphs:\n",
    "        btwnness.append(k.betweenness(weights='weight',nobigint=False))\n",
    "        count+=1\n",
    "        if count%10==0:\n",
    "            print(count)\n",
    "        \n",
    "    pd_btw = pd.DataFrame()\n",
    "    pd_btw['ids'] = graphs[0].vs()['id']\n",
    "    c=0\n",
    "    for j in snapshots:\n",
    "        pd_btw[j] = btwnness[c]\n",
    "        graphs[c].vs['betw'] = btwnness[c]\n",
    "        c+=1  \n",
    "    pd_btw.to_csv('results_3/betweenness_rail_r/'+city+'_carconstrictedbike_poi_railwaystation_random'+'_betwnns.csv')\n",
    "\n",
    "    for gr in graphs:\n",
    "        eff_global.append(calculate_efficiency_global(gr))\n",
    "        eff_local.append(calculate_efficiency_local(gr))\n",
    "        clustering_10.append(center_drift_weighted(gr,90))\n",
    "        clustering_5.append(center_drift_weighted(gr,95))\n",
    "        clustering_3.append(center_drift_weighted(gr,97))\n",
    "        anisotropy_10.append(bet_anisotropy_weighted(gr,90))\n",
    "        anisotropy_5.append(bet_anisotropy_weighted(gr,95))\n",
    "        anisotropy_3.append(bet_anisotropy_weighted(gr,97))\n",
    "        directness.append(calculate_directness(gr))\n",
    "\n",
    "\n",
    "        \n",
    "    pd_metrics = pd.DataFrame()\n",
    "\n",
    "    pd_metrics['snapshots'] = snapshots\n",
    "    metrics = ['eff_global','eff_local','clustering_10','clustering_5','clustering_3','anisotropy_10','anisotropy_5','anisotropy_3','directness']\n",
    "    values = [eff_global,eff_local,clustering_10,clustering_5,clustering_3,anisotropy_10,anisotropy_5,anisotropy_3,directness]\n",
    "\n",
    "    c_2 = 0\n",
    "    for m in metrics:\n",
    "\n",
    "        pd_metrics[m] = values[c_2]\n",
    "        c_2+=1\n",
    "        \n",
    "    pd_metrics.to_csv('results_3/metrics_rail_r/'+city+'_carconstrictedbike_poi_railwaystation_random.csv')\n",
    "    \n",
    "    print(city)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_efficiency_global(G, numnodepairs = 500, normalized = True):\n",
    "    \"\"\"Calculates global network efficiency.\n",
    "    If there are more than numnodepairs nodes, measure over pairings of a \n",
    "    random sample of numnodepairs nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    if G is None: return 0\n",
    "    if G.vcount() > numnodepairs:\n",
    "        nodeindices = random.sample(list(G.vs.indices), numnodepairs)\n",
    "    else:\n",
    "        nodeindices = list(G.vs.indices)\n",
    "    d_ij = G.shortest_paths(source = nodeindices, target = nodeindices, weights = \"weight\")\n",
    "    d_ij = [item for sublist in d_ij for item in sublist] # flatten\n",
    "    EG = sum([1/d for d in d_ij if d != 0])\n",
    "    if not normalized: return EG\n",
    "    pairs = list(itertools.permutations(nodeindices, 2))\n",
    "    if len(pairs) < 1: return 0\n",
    "    l_ij = haversine_vector([(G.vs[p[0]][\"y\"], G.vs[p[0]][\"x\"]) for p in pairs],\n",
    "                            [(G.vs[p[1]][\"y\"], G.vs[p[1]][\"x\"]) for p in pairs])\n",
    "    EG_id = sum([1/l for l in l_ij if l != 0])\n",
    "    # if (EG / EG_id) > 1: # This should not be allowed to happen!\n",
    "    #     pp.pprint(d_ij)\n",
    "    #     pp.pprint(l_ij)\n",
    "    #     pp.pprint([e for e in G.es])\n",
    "    #     print(pairs)\n",
    "    #     print([(G.vs[p[0]][\"x\"], G.vs[p[0]][\"y\"]) for p in pairs],\n",
    "    #                         [(G.vs[p[1]][\"x\"], G.vs[p[1]][\"y\"]) for p in pairs])\n",
    "    #     print(EG, EG_id)\n",
    "    #     sys.exit()\n",
    "    # assert EG / EG_id <= 1, \"Normalized EG > 1. This should not be possible.\"\n",
    "    return EG / EG_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clustering\n",
    "def center_drift_weighted(g,percentile):\n",
    "    bet_thresh=np.percentile(g.vs['betw'],percentile)\n",
    "    high_locs_x=np.array([v['x'] for v in g.vs if v['betw']>bet_thresh])\n",
    "    high_locs_y=np.array([v['y'] for v in g.vs if v['betw']>bet_thresh])\n",
    "    #mean_weight=np.mean([w for w in g.es['weight']])\n",
    "    radii=np.sqrt((high_locs_x-np.mean(high_locs_x))**2+(high_locs_y-np.mean(high_locs_y))**2)\n",
    "    all_radii=np.sqrt((g.vs['x']-np.mean(high_locs_x))**2+(g.vs['y']-np.mean(high_locs_y))**2)\n",
    "    N=float(g.vcount())\n",
    "    D=(np.ptp(g.vs['x'])+np.ptp(g.vs['y']))/2.\n",
    "    #return np.std(radii)/np.mean(radii)\n",
    "    return np.mean(radii)/np.mean(all_radii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bet_anisotropy_weighted(g,percentile):\n",
    "    bet_thresh=np.percentile(g.vs['betw'],percentile)\n",
    "    high_locs_x=np.array([v['x'] for v in g.vs if v['betw']>bet_thresh])\n",
    "    high_locs_y=np.array([v['y'] for v in g.vs if v['betw']>bet_thresh])\n",
    "    cov_mat=np.cov(high_locs_x,high_locs_y)\n",
    "    eigs=np.linalg.eig(cov_mat)[0]\n",
    "    max_eig=max(eigs)\n",
    "    min_eig=min(eigs)\n",
    "    return np.sqrt(min_eig/max_eig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_efficiency_local(G, numnodepairs = 500, normalized = True):\n",
    "    \"\"\"Calculates local network efficiency.\n",
    "    If there are more than numnodepairs nodes, measure over pairings of a \n",
    "    random sample of numnodepairs nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    if G is None: return 0\n",
    "    if G.vcount() > numnodepairs:\n",
    "        nodeindices = random.sample(list(G.vs.indices), numnodepairs)\n",
    "    else:\n",
    "        nodeindices = list(G.vs.indices)\n",
    "    EGi = []\n",
    "    vcounts = []\n",
    "    ecounts = []\n",
    "    for i in nodeindices:\n",
    "        if len(G.neighbors(i)) > 1: # If we have a nontrivial neighborhood\n",
    "            G_induced = G.induced_subgraph(G.neighbors(i))\n",
    "            EGi.append(calculate_efficiency_global(G_induced, numnodepairs, normalized))\n",
    "    return listmean(EGi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listmean(lst): \n",
    "    try: return sum(lst) / len(lst)\n",
    "    except: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_directness(G, numnodepairs = 500):\n",
    "    \"\"\"Calculate directness on G over all connected node pairs in indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    indices = random.sample(list(G.vs), min(numnodepairs, len(G.vs)))\n",
    "\n",
    "    poi_edges = []\n",
    "    v1 = []\n",
    "    v2 = []\n",
    "    total_distance_haversine = 0\n",
    "    for c, v in enumerate(indices):\n",
    "        poi_edges.append(G.get_shortest_paths(v, indices[c:], weights = \"weight\", output = \"epath\"))\n",
    "        temp = G.get_shortest_paths(v, indices[c:], weights = \"weight\", output = \"vpath\")\n",
    "        if len(temp) > 1:\n",
    "            total_distance_haversine += sum(haversine_vector([(G.vs[t[0]][\"y\"], G.vs[t[0]][\"x\"]) for t in temp if len(t) !=0], [(G.vs[t[-1]][\"y\"], G.vs[t[-1]][\"x\"]) for t in temp if len(t) !=0]))\n",
    "    \n",
    "    total_distance_network = 0\n",
    "    for paths_e in poi_edges:\n",
    "        for path_e in paths_e:\n",
    "            # Sum up distances of path segments from first to last node\n",
    "            total_distance_network += sum([G.es[e]['weight'] for e in path_e])\n",
    "    \n",
    "    return total_distance_haversine / total_distance_network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
